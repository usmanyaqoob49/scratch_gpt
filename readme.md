# scratch gpt
I tried my best to do functional coding so that anyone can use this project for their usecase by using their own datasets. I tired my best to keep it clean but I would love to see it more improved :) 
Project that has:
        - Data preparation codes that we need for using the coding the LLM like GPT-2 from scratch in src.data_preparation.
        - Scratch Codes of Scaled Self Attention, Causal Self Attention, Multihead Attention in src.attention_mechanism.
        - Transformer scratch code in src.transformer.
        - GPT-2 124M architecture developed from scratch using pytorch in src.gpt.
        - Functions and data loaders for pretraining of gpt-2 using any tectual dataset in src.pretraining.
        - 